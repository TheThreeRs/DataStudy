{"paragraphs":[{"text":"%md\n## Scalable Analytics with Apache Hadoop and Spark V2\n**Version 2.0 08-Jul-2020**\n\nThis notebook is part of the course *Scalable Analytics with Apache Hadoop and Spark*. We will use various methods to try and predict airline delays from Chicago (ORD) airport in the US.\n\n[Additional course information ](https://www.clustermonkey.net/scalable-analytics)\n\nBased on [Data Science with Hadoop - predicting airline delays - part 1]  (https://nbviewer.jupyter.org/github/ofermend/IPython-notebooks/blob/master/blog-part-1.ipynb) by Ofer Mendelevitch\n\nThe goal of this notebook is to show a variety of ways and toosl (Python,Hive, PySpark) to do data analytics.\nSimilar to Ofer's approach we will learn:\n\n1. How to use Python and Matplotlib to explore the raw dataset\n2. How to use Hive and Python to transform our raw data into a feature matrix\n3. How to use Python's excellent Scikit-learn machine learning library for building a predictive model.\n4. And, How to use PySpark to create and run a model\n\n### Updates:\n\n* \n","user":"deadline","dateUpdated":"2020-07-09T23:01:36-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157921_589418606","id":"20190715-110940_1075050056","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:01:36-0400","dateFinished":"2020-07-09T23:01:36-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5626","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Scalable Analytics with Apache Hadoop and Spark V2</h2>\n<p><strong>Version 2.0 08-Jul-2020</strong></p>\n<p>This notebook is part of the course <em>Scalable Analytics with Apache Hadoop and Spark</em>. We will use various methods to try and predict airline delays from Chicago (ORD) airport in the US.</p>\n<p><a href=\"https://www.clustermonkey.net/scalable-analytics\">Additional course information </a></p>\n<p>Based on <a href=\"https://nbviewer.jupyter.org/github/ofermend/IPython-notebooks/blob/master/blog-part-1.ipynb\">Data Science with Hadoop - predicting airline delays - part 1</a> by Ofer Mendelevitch</p>\n<p>The goal of this notebook is to show a variety of ways and toosl (Python,Hive, PySpark) to do data analytics.<br/>Similar to Ofer&rsquo;s approach we will learn:</p>\n<ol>\n  <li>How to use Python and Matplotlib to explore the raw dataset</li>\n  <li>How to use Hive and Python to transform our raw data into a feature matrix</li>\n  <li>How to use Python&rsquo;s excellent Scikit-learn machine learning library for building a predictive model.</li>\n  <li>And, How to use PySpark to create and run a model</li>\n</ol>\n<h3>Updates:</h3>\n<p>*</p>\n</div>"}]}},{"text":"%md\n\nThe airline delay dataset is  available [here] (http://stat-computing.org/dataexpo/2009/the-data.html) This dataset includes details about flights in the US from the years 1987-2008. Every row in the dataset includes 29 variables:\n\n_ |\tName|\tDescription\n --- | --- | ---\n1|\tYear|\t1987-2008\n2 |\tMonth |\t1-12\n3 |\tDayofMonth |\t1-31\n4 |\tDayOfWeek |\t1 (Monday) - 7 (Sunday)\n5 |\tDepTime\tactual | departure time (local, hhmm)\n6 |\tCRSDepTime |\tscheduled departure time (local, hhmm)\n7 |\tArrTime\tactual | arrival time (local, hhmm)\n8 |\tCRSArrTime |\tscheduled arrival time (local, hhmm)\n9 |\tUniqueCarrier |\tunique carrier code\n10 |\tFlightNum\t |flight number\n11 |\tTailNum\tplane | tail number\n12 |\tActualElapsedTime |\tin minutes\n13 |\tCRSElapsedTime |\tin minutes\n14 |\tAirTime |\tin minutes\n15 |\tArrDelay |\tarrival delay, in minutes\n16 |\tDepDelay |\tdeparture delay, in minutes\n17 |\tOrigin |\torigin\n18 |\tDest |\tdestination\n19 |\tDistance |\tin miles\n20 |\tTaxiIn\ttaxi in  |time, in minutes\n21 |\tTaxiOut\ttaxi out | time in minutes\n22 |\tCancelled |\twas the flight cancelled?\n23 |\tCancellationCode |\treason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n24 |\tDiverted |\t1 = yes, 0 = no\n25 |\tCarrierDelay |\tin minutes\n26 |\tWeatherDelay |\tin minutes\n27 |\tNASDelay |\tin minutes\n28 |\tSecurityDelay |\tin minutes\n29 |\tLateAircraftDelay |\tin minutes\n","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157924_588264359","id":"20190716-080241_655610682","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5627"},{"text":"%md\n### Download Data\nNote: this step may not be needed, check to see if the data exists in your HDFS file system.\nTo download and add data to HDFS, perform these steps as user 'hdfs' on your Hadoop cluster.\n\n1. Make local directories for data\n```  \n  mkdir flights\n  mkdir weather\n```\n2. Make directories in HDFS\n```\n  hdfs dfs -mkdir -p /data/flights\n  hdfs dfs -mkdir -p /data/weather\n```\n3. Download, extract, and move flight data into HDFS\n```\n  cd flights/\n  wget http://stat-computing.org/dataexpo/2009/2008.csv.bz2\n  wget http://stat-computing.org/dataexpo/2009/2007.csv.bz2\n  bzip2 -d 2007.csv.bz2\n  bzip2 -d 2008.csv.bz2\n  hdfs dfs -put 200*.csv /data/flights\n```\n4. Download, extract, and move weather  data into HDFS\n```\n  cd ../weather/\n  wget ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2007.csv.gz\n  wget ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2008.csv.gz\n  gunzip 2007.csv.gz\n  gunzip 2008.csv.gz\n  hdfs dfs -put 200*.csv /data/weather\n  cd ..\n```\n\n#### File size in HDFS after download and decompression\n* /data/flights/2007.csv 671M\n* /data/flights/2008.csv 658M \n* /data/weather/2007.cvs 1.3G\n* /data/weather/2008.cvs 1.3G\n\n","dateUpdated":"2020-07-09T23:01:41-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157927_588649108","id":"20190715-111310_266286812","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5628","user":"deadline","dateFinished":"2020-07-09T23:01:41-0400","dateStarted":"2020-07-09T23:01:41-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Download Data</h3>\n<p>Note: this step may not be needed, check to see if the data exists in your HDFS file system.<br/>To download and add data to HDFS, perform these steps as user &lsquo;hdfs&rsquo; on your Hadoop cluster.</p>\n<ol>\n  <li>\n  <p>Make local directories for data</p>\n  <pre><code>  mkdir flights\n  mkdir weather\n</code></pre></li>\n  <li>\n  <p>Make directories in HDFS</p>\n  <pre><code>  hdfs dfs -mkdir -p /data/flights\n  hdfs dfs -mkdir -p /data/weather\n</code></pre></li>\n  <li>\n  <p>Download, extract, and move flight data into HDFS</p>\n  <pre><code>  cd flights/\n  wget http://stat-computing.org/dataexpo/2009/2008.csv.bz2\n  wget http://stat-computing.org/dataexpo/2009/2007.csv.bz2\n  bzip2 -d 2007.csv.bz2\n  bzip2 -d 2008.csv.bz2\n  hdfs dfs -put 200*.csv /data/flights\n</code></pre></li>\n  <li>\n  <p>Download, extract, and move weather data into HDFS</p>\n  <pre><code>  cd ../weather/\n  wget ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2007.csv.gz\n  wget ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2008.csv.gz\n  gunzip 2007.csv.gz\n  gunzip 2008.csv.gz\n  hdfs dfs -put 200*.csv /data/weather\n  cd ..\n</code></pre></li>\n</ol>\n<h4>File size in HDFS after download and decompression</h4>\n<ul>\n  <li>/data/flights/2007.csv 671M</li>\n  <li>/data/flights/2008.csv 658M</li>\n  <li>/data/weather/2007.cvs 1.3G</li>\n  <li>/data/weather/2008.cvs 1.3G</li>\n</ul>\n</div>"}]}},{"text":"%md\nWe start by importing some useful python libraries that we will need later like Pandas, Numpy, Scikit-learn and Matplotlib.","dateUpdated":"2020-07-09T23:01:46-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157930_587494862","id":"20190716-080103_450856631","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5629","user":"deadline","dateFinished":"2020-07-09T23:01:46-0400","dateStarted":"2020-07-09T23:01:46-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We start by importing some useful python libraries that we will need later like Pandas, Numpy, Scikit-learn and Matplotlib.</p>\n</div>"}]}},{"text":"%python\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sys\nimport random\nimport numpy as np\nfrom sklearn import linear_model, metrics, svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Print the Python version (Anaconda)\nprint(sys.version)\n","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157933_584801619","id":"20190715-111743_870214658","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:11:07-0400","dateFinished":"2020-07-09T21:11:19-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5630"},{"text":"%md\nWe now define a utility function to read an HDFS file into a Pandas dataframe using Pydoop. Pydoop is a package that provides a Python API for Hadoop MapReduce and HDFS.\n\nPydoop's hdfs.open() function reads a single file from HDFS. However many HDFS output files are actually multi-part files, so our read_csv_from_hdfs() function uses hdfs.ls() to grab all the needed file names, and then read each one separately. Finally, it concatenates the resulting Pandas dataframes of each file into a Pandas dataframe.\n","dateUpdated":"2020-07-09T23:01:49-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157938_596728835","id":"20190716-082017_858176609","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5631","user":"deadline","dateFinished":"2020-07-09T23:01:49-0400","dateStarted":"2020-07-09T23:01:49-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We now define a utility function to read an HDFS file into a Pandas dataframe using Pydoop. Pydoop is a package that provides a Python API for Hadoop MapReduce and HDFS.</p>\n<p>Pydoop&rsquo;s hdfs.open() function reads a single file from HDFS. However many HDFS output files are actually multi-part files, so our read_csv_from_hdfs() function uses hdfs.ls() to grab all the needed file names, and then read each one separately. Finally, it concatenates the resulting Pandas dataframes of each file into a Pandas dataframe.</p>\n</div>"}]}},{"text":"%python\n\n# function to read HDFS file into dataframe using PyDoop\nimport pydoop.hdfs as hdfs\ndef read_csv_from_hdfs(path, cols, col_types=None):\n  files = hdfs.ls(path);\n  pieces = []\n  for f in files:\n    fhandle = hdfs.open(f)\n    pieces.append(pd.read_csv(fhandle, names=cols, dtype=col_types))\n    fhandle.close()\n  return pd.concat(pieces, ignore_index=True)","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157942_595189840","id":"20190715-111826_1381366699","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:12:32-0400","dateFinished":"2020-07-09T21:12:33-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5632"},{"text":"%md\nNext, read the raw data for 2007 from HDFS into a Pandas dataframe.The utility function read_csv_from_hdfs()reads the csv data and provide it with column names since this is a raw file. To check our result we report the shape of the data. We can ignore the Hadoop warnings.\n","dateUpdated":"2020-07-09T23:01:53-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157946_593650844","id":"20190716-082132_1793543959","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5633","user":"deadline","dateFinished":"2020-07-09T23:01:53-0400","dateStarted":"2020-07-09T23:01:53-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Next, read the raw data for 2007 from HDFS into a Pandas dataframe.The utility function read_csv_from_hdfs()reads the csv data and provide it with column names since this is a raw file. To check our result we report the shape of the data. We can ignore the Hadoop warnings.</p>\n</div>"}]}},{"text":"%python\n\n# read 2007 year file\ncols = ['year', 'month', 'day', 'dow', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'Carrier', 'FlightNum', \n        'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', \n        'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', \n        'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay'];\nflt_2007 = read_csv_from_hdfs('/data/flights/2007/2007.csv', cols)\n\nflt_2007.shape","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157950_592111848","id":"20190715-135456_1892521233","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:14:49-0400","dateFinished":"2020-07-09T21:15:15-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5634"},{"text":"%md\nWe see 7,453,216 flights in 2007 and 29 variables (as expected).\n\nOur \"target\" variable will be DepDelay (scheduled departure delay in minutes). To build a classifier, we further refine our target variable into a binary variable by defining a \"delay\" as having 15 mins or more of delay, and \"non-delay\" otherwise. We thus create a new binary variable that we name 'DepDelayed'.\n\nLet's look at some basic statistics,  to flights originating from ORD:\n","dateUpdated":"2020-07-09T23:01:57-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157953_675602360","id":"20190716-082345_722173524","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5635","user":"deadline","dateFinished":"2020-07-09T23:01:57-0400","dateStarted":"2020-07-09T23:01:57-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We see 7,453,216 flights in 2007 and 29 variables (as expected).</p>\n<p>Our &ldquo;target&rdquo; variable will be DepDelay (scheduled departure delay in minutes). To build a classifier, we further refine our target variable into a binary variable by defining a &ldquo;delay&rdquo; as having 15 mins or more of delay, and &ldquo;non-delay&rdquo; otherwise. We thus create a new binary variable that we name &lsquo;DepDelayed&rsquo;.</p>\n<p>Let&rsquo;s look at some basic statistics, to flights originating from ORD:</p>\n</div>"}]}},{"text":"%python\ndf = flt_2007[flt_2007['Origin']=='ORD'].dropna(subset=['DepDelay'])\ndf['DepDelayed'] = df['DepDelay'].apply(lambda x: x>=15)\nprint(\"total flights: \" + str(df.shape[0]))\nprint(\"total delays: \" + str(df['DepDelayed'].sum()))\n","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157956_674448113","id":"20190715-144200_1209558881","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:15:24-0400","dateFinished":"2020-07-09T21:15:26-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5636"},{"text":"%md\nWe now have 359,169 flights with 109,346 delays. Next we explore some of the features. The following displays the delayed flights by month:\n","dateUpdated":"2020-07-09T23:02:01-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157958_675217611","id":"20190716-083313_1614270795","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5637","user":"deadline","dateFinished":"2020-07-09T23:02:01-0400","dateStarted":"2020-07-09T23:02:01-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We now have 359,169 flights with 109,346 delays. Next we explore some of the features. The following displays the delayed flights by month:</p>\n</div>"}]}},{"text":"%python\n# Select a Pandas dataframe with flight originating from ORD\n\n# Compute average number of delayed flights per month\ngrouped = df[['DepDelayed', 'month']].groupby('month').mean()\n\n# plot average delays by month\ngrouped.plot(kind='bar')\n","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157960_672909117","id":"20190715-144405_635139247","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:15:35-0400","dateFinished":"2020-07-09T21:15:37-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5638"},{"text":"%md\nAs expected we see big differences in the delays over months. (which often reflect our experience with holiday months)\nNext consider the hour-of-day:\n","dateUpdated":"2020-07-09T23:02:06-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157963_673293866","id":"20190716-083836_970925767","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5639","user":"deadline","dateFinished":"2020-07-09T23:02:06-0400","dateStarted":"2020-07-09T23:02:06-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As expected we see big differences in the delays over months. (which often reflect our experience with holiday months)<br/>Next consider the hour-of-day:</p>\n</div>"}]}},{"text":"%python\n# Compute average number of delayed flights by hour\ndf['hour'] = df['CRSDepTime'].map(lambda x: int(str(int(x)).zfill(4)[:2]))\ngrouped = df[['DepDelayed', 'hour']].groupby('hour').mean()\n\n# plot average delays by hour of day\ngrouped.plot(kind='bar')\n","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157965_670985373","id":"20190715-144853_802137704","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:15:43-0400","dateFinished":"2020-07-09T21:15:45-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5640"},{"text":"%md\nAs our experience tells us flights tend to be delayed later in the day. Now let's look at delays by carrier:\n","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157966_672139620","id":"20190716-084109_11484781","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5641"},{"text":"%python\n# Compute average number of delayed flights per carrier\ngrouped1 = df[['DepDelayed', 'Carrier']].groupby('Carrier').filter(lambda x: len(x)>10)\ngrouped2 = grouped1.groupby('Carrier').mean()\ncarrier = grouped2.sort_values(['DepDelayed'], ascending=False)\n\n# display top 15 destination carriers by delay (from ORD)\ncarrier[:15].plot(kind='bar')\n","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157968_682143091","id":"20190715-144951_470679494","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:15:49-0400","dateFinished":"2020-07-09T21:15:51-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5642"},{"text":"%md\nWe now would like to build a feature matrix for our predictive model.\n\nLet's look at possible predictive variables for our model:\n\n1. month: winter months should have more delays than summer months\n2. day of month: this is likely not a very predictive variable, but let's keep it in anyway\n3. day of week: weekend vs. weekday\n4. hour of the day: later hours tend to have more delays\n5. Carrier: we might expect some carriers to be more prone to delays than others\n6. Destination airport: we expect some airports to be more prone to delays than others\n7. Distance: interesting to see if this variable is a good predictor of delay\n\nAnother **generated** feature is the number of days from closest national holiday, with the assumption that holidays tend to be associated with more delays.\n\nWe implement this \"feature generation\" process using Hive and a simple Python mapper function. First, let's create our Python mapper functions using a shell *heredoc* method. (The *heredoc* method allows the Notebook to write a file from the notebook to the local working directory. In the case below we will create the file `feature-gen.py`)\n","user":"deadline","dateUpdated":"2020-07-09T23:02:15-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157970_682912589","id":"20190716-084235_1849126309","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:02:15-0400","dateFinished":"2020-07-09T23:02:15-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5643","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We now would like to build a feature matrix for our predictive model.</p>\n<p>Let&rsquo;s look at possible predictive variables for our model:</p>\n<ol>\n  <li>month: winter months should have more delays than summer months</li>\n  <li>day of month: this is likely not a very predictive variable, but let&rsquo;s keep it in anyway</li>\n  <li>day of week: weekend vs. weekday</li>\n  <li>hour of the day: later hours tend to have more delays</li>\n  <li>Carrier: we might expect some carriers to be more prone to delays than others</li>\n  <li>Destination airport: we expect some airports to be more prone to delays than others</li>\n  <li>Distance: interesting to see if this variable is a good predictor of delay</li>\n</ol>\n<p>Another <strong>generated</strong> feature is the number of days from closest national holiday, with the assumption that holidays tend to be associated with more delays.</p>\n<p>We implement this &ldquo;feature generation&rdquo; process using Hive and a simple Python mapper function. First, let&rsquo;s create our Python mapper functions using a shell <em>heredoc</em> method. (The <em>heredoc</em> method allows the Notebook to write a file from the notebook to the local working directory. In the case below we will create the file <code>feature-gen.py</code>)</p>\n</div>"}]}},{"text":"%sh\n# This paragraph uses the \"sh\" command to run a shell command in the working directory.\n# The command below will \"print the working directoryy path\" (pwd) and\n# list the files (ls)\n\ndate\npwd\nls\n","user":"deadline","dateUpdated":"2020-07-09T23:01:17-0400","config":{"enabled":true,"editorMode":"ace/mode/sh","results":{},"editorHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157971_682527840","id":"20190716-132744_253871478","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:23:46-0400","dateFinished":"2020-07-09T21:23:47-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5644"},{"text":"%sh\n# user heredoc to write feature-gen.py to the working directory\n cat << EOF >feature-gen.py\nimport sys\nimport datetime\nfrom datetime import date\n\n# this array defines the dates of holiday in 2007 and 2008\nholidays = [\n        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n     ]\n# get number of days from nearest holiday\ndef days_from_nearest_holiday(year, month, day):\n    d = date(year, month, day)\n    x = [(abs(d-h)).days for h in holidays]\n    return min(x)\n\n\nfor line in sys.stdin:\n  line = line.strip()\n  delay, year, month, day, dayofweek, hour_mins, distance, carrier, dest, days_from_holiday = line.split('\\t')\n  print '\\t'.join([delay, month, day, dayofweek, str(hour_mins.zfill(4)[:2]), distance, carrier, dest, str(days_from_nearest_holiday(int(year), int(month), int(day)))])\n\nEOF\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/sh","results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157972_680604095","id":"20190715-150134_2025525704","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5645"},{"text":"%md\nOur Hive operation consist of a series of Hive scripts that do the following for years 2007 and 2008:\n\n1. Load the each flight data into an external table\n2. Create a fmatrix_2007_raw table with only the features we need and flights that were not cancelled and originating in ORD (Chicago)\n3. Use the Python mapper program to transform the raw data (clean up the data and calculate `days_from_holiday`)\n4. Write the table as a CSV file in HDFS \n\n**Note:** these Hive script is not working when run though this notebook. We will run this from the command line. They are included as  in *heredoc* so they can be saved in the note book and run from the command line using `hive -f filename/hql` (working on a solution)\n","user":"deadline","dateUpdated":"2020-07-09T23:02:30-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157973_680219347","id":"20190716-085140_912506178","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:02:30-0400","dateFinished":"2020-07-09T23:02:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5646","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Our Hive operation consist of a series of Hive scripts that do the following for years 2007 and 2008:</p>\n<ol>\n  <li>Load the each flight data into an external table</li>\n  <li>Create a fmatrix_2007_raw table with only the features we need and flights that were not cancelled and originating in ORD (Chicago)</li>\n  <li>Use the Python mapper program to transform the raw data (clean up the data and calculate <code>days_from_holiday</code>)</li>\n  <li>Write the table as a CSV file in HDFS</li>\n</ol>\n<p><strong>Note:</strong> these Hive script is not working when run though this notebook. We will run this from the command line. They are included as in <em>heredoc</em> so they can be saved in the note book and run from the command line using <code>hive -f filename/hql</code> (working on a solution)</p>\n</div>"}]}},{"text":"%sh\n# user heredoc to local directory. Run as \"hive -f hive-script-1.hql\"\n cat << EOF >hive-script-1.hql\n -- SCRIPT 1: CREATE HIVE EXTERNAL TABLES WITH RAW DATA\n-- Create two tables from raw data for years 2007 and 2008\n\nCREATE EXTERNAL TABLE flights_2007 (\nYear int,Month int,DayofMonth int,DayOfWeek int,\nDepTime string, CRSDepTime string, ArrTime string, CRSArrTime string,\nUniqueCarrier string, FlightNum int, TailNum string,\nActualElapsedTime int, CRSElapsedTime int, AirTime int,\nArrDelay int, DepDelay int, Origin string, Dest string, Distance int,\nTaxiIn int, TaxiOut int, Cancelled int, CancellationCode int,\nDiverted int, CarrierDelay int, WeatherDelay int,\nNASDelay int, SecurityDelay int, LateAircraftDelay int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE\nLOCATION '/data/flights/2007'\nTBLPROPERTIES('skip.header.line.count'='1','external.table.purge'='false');\n\nCREATE EXTERNAL TABLE flights_2008 (\nYear int,Month int,DayofMonth int,DayOfWeek int,\nDepTime string, CRSDepTime string, ArrTime string, CRSArrTime string,\nUniqueCarrier string, FlightNum int, TailNum string,\nActualElapsedTime int, CRSElapsedTime int, AirTime int,\nArrDelay int, DepDelay int, Origin string, Dest string, Distance int,\nTaxiIn int, TaxiOut int, Cancelled int, CancellationCode int,\nDiverted int, CarrierDelay int, WeatherDelay int,\nNASDelay int, SecurityDelay int, LateAircraftDelay int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE\nLOCATION '/data/flights/2008'\nTBLPROPERTIES('skip.header.line.count'='1','external.table.purge'='false');\n\n--Check if there is data\nSELECT COUNT(*) FROM flights_2007;\nSELECT * FROM flights_2007 LIMIT 5;\nSELECT COUNT(*) FROM flights_2008;\nSELECT * FROM flights_2008 LIMIT 5;\nEOF","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":true,"editorMode":"ace/mode/sh","results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157974_681373593","id":"20190715-145039_1112446165","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5647"},{"text":"%sh\n# user heredoc to local directory. Run as \"hive -f hive-script-2.hql\"\n cat << EOF >hive-script-2.hql\n-- SCRIPT 2 CREATE FEATURE MATRIX AND SAVE AS CSV FILE FOR BOTH 2007 and 2008\n\n-- Create the feature matrix raw files\nCREATE TABLE fmatrix_2007_raw (delay int, year int, month int, day int, dayofweek int, hour_mins string, distance int, carrier string, dest string, days_from_holiday int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE fmatrix_2008_raw (delay int, year int, month int, day int, dayofweek int, hour_mins string, distance int, carrier string, dest string, days_from_holiday int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- Insert only needed data, note colums need to be lower case, hive eliminates upper case\n-- Data are delay, year, month, day, dayofweek, hour_mins, distance, carier, destination\n\nINSERT INTO TABLE fmatrix_2007_raw(delay, year, month, day, dayofweek, hour_mins, distance, carrier,dest)\nSELECT depdelay, year, month, dayofmonth, dayofWeek, crsdeptime,distance, uniquecarrier, dest FROM flights_2007\nWHERE origin='ORD' and cancelled=0 and depdelay is not NULL and depdelay>0;\n\nINSERT INTO TABLE fmatrix_2008_raw(delay, year, month, day, dayofweek, hour_mins, distance, carrier,dest)\nSELECT depdelay, year, month, dayofmonth, dayofWeek, crsdeptime,distance, uniquecarrier, dest FROM flights_2008\nWHERE origin='ORD' and cancelled=0 and depdelay is not NULL and depdelay>0;\n\n-- check results\nSELECT * FROM fmatrix_2007_raw LIMIT 5;\nSELECT COUNT(*) FROM fmatrix_2007_raw;\nSELECT * FROM fmatrix_2008_raw LIMIT 5;\nSELECT COUNT(*) FROM fmatrix_2008_raw;\n\n-- The raw data need to be converted from hour-minutes to hour \n-- and the days_from_holiday. This idone using a Python mapper function\n-- with Hive\n\n-- Create the tables for new results\n\nCREATE TABLE fmatrix_2007 (delay int, month int, day int, dayofweek int, hour string, distance int, carrier string, dest string, days_from_holiday int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE fmatrix_2008 (delay int, month int, day int, dayofweek int, hour string, distance int, carrier string, dest string, days_from_holiday int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- The python file (feature-gn.py) is assumed to be present in\n-- the directory that is runnign the script and it must be added to Hive\n-- Note if you need to change this file delete and then reload it\n-- delete FILE feature-gen.py\nadd FILE feature-gen.py;\n\n-- Do the transformation\nINSERT OVERWRITE TABLE fmatrix_2007\nSELECT\n  TRANSFORM (delay, year, month, day, dayofweek, hour_mins, distance, carrier, dest, days_from_holiday)\n  USING 'python feature-gen.py'\n  AS (delay, month, day, dayofweek, hour, distance, carrier, dest, days_from_holiday)\nFROM fmatrix_2007_raw;\n\nINSERT OVERWRITE TABLE fmatrix_2008\nSELECT\n  TRANSFORM (delay, year, month, day, dayofweek, hour_mins, distance, carrier, dest, days_from_holiday)\n  USING 'python feature-gen.py'\n  AS (delay, month, day, dayofweek, hour, distance, carrier, dest, days_from_holiday)\nFROM fmatrix_2008_raw;\n\n-- check result\nSELECT COUNT(*) FROM fmatrix_2007;\nSELECT * FROM fmatrix_2007 LIMIT 10;\nSELECT COUNT(*) FROM fmatrix_2008;\nSELECT * FROM fmatrix_2008 LIMIT 10;\n\n\n-- Save the tables as CSV to HDFS for other processing\nINSERT OVERWRITE DIRECTORY 'airline/fmatrix/ord_2007_1' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM fmatrix_2007 ;\n\nINSERT OVERWRITE DIRECTORY 'airline/fmatrix/ord_2008_1' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM fmatrix_2008 ;\nEOF\n\n\n","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594345213921_-1310796","id":"20200709-214013_1138398660","dateCreated":"2020-07-09T21:40:13-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5648"},{"text":"%md\n\nLet's take a look at data in HDFS using `%sh` interpreter and `hdfs dfs` commands. \n","user":"deadline","dateUpdated":"2020-07-09T23:02:37-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157975_680988844","id":"20190716-092308_194144036","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:02:37-0400","dateFinished":"2020-07-09T23:02:37-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5649","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Let&rsquo;s take a look at data in HDFS using <code>%sh</code> interpreter and <code>hdfs dfs</code> commands.</p>\n</div>"}]}},{"text":"%sh \necho \"2007 ORD data in HDFS\"\nhdfs dfs -ls airline/fmatrix/ord_2007_1\necho \"2008 ORD data in HDFS\"\nhdfs dfs -ls airline/fmatrix/ord_2008_1\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/sh","results":{},"editorHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157976_679065100","id":"20190716-092018_1008065710","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5650"},{"text":"%md\n\nWe have the files in `ord_2007_1` and `ord_2008_1` under `airline/fmatrix` folder in HDFS (these contain the 359,169 ORD flights). Let's read those files into Python (using `read_csv_from_hdfs()` defined above), and prepare the training and testing (validation) datasets as Pandas DataFrame objects.\n\nThere are comments in the code that begin with `# VIEW`. The lines after this comment can be “uncommented” and used to view the data as the analysis progresses.\n\nInitially, we use only the numerical variables:\n","user":"deadline","dateUpdated":"2020-07-09T23:02:45-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157977_678680351","id":"20190716-091705_1541223639","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:02:45-0400","dateFinished":"2020-07-09T23:02:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5651","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We have the files in <code>ord_2007_1</code> and <code>ord_2008_1</code> under <code>airline/fmatrix</code> folder in HDFS (these contain the 359,169 ORD flights). Let&rsquo;s read those files into Python (using <code>read_csv_from_hdfs()</code> defined above), and prepare the training and testing (validation) datasets as Pandas DataFrame objects.</p>\n<p>There are comments in the code that begin with <code># VIEW</code>. The lines after this comment can be “uncommented” and used to view the data as the analysis progresses.</p>\n<p>Initially, we use only the numerical variables:</p>\n</div>"}]}},{"text":"%python\nfrom itertools import islice\n\n# read processed file (from above Hive script)\ncols = ['delay', 'month', 'day', 'dow', 'hour', 'distance', 'carrier', 'dest', 'days_from_holiday']\ncol_types = {'delay': int, 'month': int, 'day': int, 'dow': int, 'hour': int, 'distance': int, \n             'carrier': str, 'dest': str, 'days_from_holiday': int}\ndata_2007 = read_csv_from_hdfs('airline/fmatrix/ord_2007_1', cols, col_types)\ndata_2008 = read_csv_from_hdfs('airline/fmatrix/ord_2008_1', cols, col_types)\n\n# Create training set and test set (numberic variables only)\ncols = ['month', 'day', 'dow', 'hour', 'distance', 'days_from_holiday']\ntrain_y = data_2007['delay'] >= 15\ntrain_x = data_2007[cols]\n\ntest_y = data_2008['delay'] >= 15\ntest_x = data_2008[cols]\n\n# VIEW look at data (first 10 lines)\nprint(train_y[:10])\nprint(train_x[:10])\nprint(train_x.shape)\n","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157978_679834598","id":"20190715-152704_1664933868","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:48:08-0400","dateFinished":"2020-07-09T21:48:08-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5652"},{"text":"%md\n### Logistic Regression and Random Forest (Iteration 1) ###\nOur data is 359,169 rows and 6 features in our model.\n\nNow we use Python's Scikit-learn machine learning package to to build two predictive models (Logistic regression and Random Forest) and compare their performance. To tell if we are making progress we print the confusion matrix, which counts the true positive, true negatives, false positives and false negatives. Then from the confusion matrix, we compute precision, recall, F1 metric and accuracy. We start with a logistic regression model and evaluate its performance on the testing dataset.\n","dateUpdated":"2020-07-09T23:02:50-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157978_679834598","id":"20190716-092406_829447660","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5653","user":"deadline","dateFinished":"2020-07-09T23:02:50-0400","dateStarted":"2020-07-09T23:02:50-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Logistic Regression and Random Forest (Iteration 1)</h3>\n<p>Our data is 359,169 rows and 6 features in our model.</p>\n<p>Now we use Python&rsquo;s Scikit-learn machine learning package to to build two predictive models (Logistic regression and Random Forest) and compare their performance. To tell if we are making progress we print the confusion matrix, which counts the true positive, true negatives, false positives and false negatives. Then from the confusion matrix, we compute precision, recall, F1 metric and accuracy. We start with a logistic regression model and evaluate its performance on the testing dataset.</p>\n</div>"}]}},{"text":"%python\n# Create logistic regression model object with L2 regularization \nclf_lr = linear_model.LogisticRegression(penalty='l2', class_weight='balanced')\n\n# Train the model using the training sets\nclf_lr.fit(train_x, train_y)\n\n# Predict output labels on test set\npr = clf_lr.predict(test_x)\n\n# display evaluation metrics\ncm = confusion_matrix(test_y, pr)\nprint(\"Confusion matrix\")\nprint(pd.DataFrame(cm))\nreport_lr = precision_recall_fscore_support(list(test_y), list(pr), average='binary')\nprint(\"\\nprecision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n        (report_lr[0], report_lr[1], report_lr[2], accuracy_score(list(test_y), list(pr))))","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157979_679449849","id":"20190715-173043_568146155","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:49:11-0400","dateFinished":"2020-07-09T21:49:12-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5654"},{"text":"%md\nLogistic regression model got overall accuracy of 57%. Now let's try Random Forest:\n","user":"deadline","dateUpdated":"2020-07-09T23:02:54-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157979_679449849","id":"20190716-092803_658853650","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:02:54-0400","dateFinished":"2020-07-09T23:02:54-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5655","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Logistic regression model got overall accuracy of 57%. Now let&rsquo;s try Random Forest:</p>\n</div>"}]}},{"text":"%python\n# Create Random Forest classifier with 50 trees\nclf_rf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\nclf_rf.fit(train_x, train_y)\n\n# Evaluate on test set\npr = clf_rf.predict(test_x)\n\n# print results\ncm = confusion_matrix(test_y, pr)\nprint(\"Confusion matrix\")\nprint(pd.DataFrame(cm))\nreport_svm = precision_recall_fscore_support(list(test_y), list(pr), average='binary')\nprint(\"\\nprecision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n        (report_svm[0], report_svm[1], report_svm[2], accuracy_score(list(test_y), list(pr))))","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157980_677526104","id":"20190715-173242_1888587451","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:49:46-0400","dateFinished":"2020-07-09T21:49:50-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5656"},{"text":"%md\n\nAs we can see, Random Forest slightly worse, and the true positives were worse\n\nWith any supervised learnign algorithm, one typically needs to choose values for the parameters of the model. For example, we chose \"L1\" regularization for the logistic regression model, and 50 trees for the Random Forest. Such choices are based on some experimentation and [hyperparameter tuning] (http://en.wikipedia.org/wiki/Hyperparameter_optimization). We are not addressing this topic in this class, although such choices (experimentation) are important to achieve the overall best model.\n","user":"deadline","dateUpdated":"2020-07-09T23:02:57-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157980_677526104","id":"20190716-092900_1395771202","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:02:57-0400","dateFinished":"2020-07-09T23:02:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5657","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As we can see, Random Forest slightly worse, and the true positives were worse</p>\n<p>With any supervised learnign algorithm, one typically needs to choose values for the parameters of the model. For example, we chose &ldquo;L1&rdquo; regularization for the logistic regression model, and 50 trees for the Random Forest. Such choices are based on some experimentation and <a href=\"http://en.wikipedia.org/wiki/Hyperparameter_optimization\">hyperparameter tuning</a>. We are not addressing this topic in this class, although such choices (experimentation) are important to achieve the overall best model.</p>\n</div>"}]}},{"text":"%md\n### One Hot Encoding (Iteration 2) ###\nIt is very common in data science to work iteratively, and improve the model with each iteration.\n\nIn this iteration, we improve our feature by converting existing variables that are categorical in nature (such as \"hour\", or \"month\") as well as categorical variables that are strings (like \"carrier\" and \"dest\"), into what is known as \"dummy variables\". Each \"dummy variable\" is a binary (0 or 1) that indicates whether a certain category value is \"on\" or \"off.\n\nscikit-learn has the OneHotEncoder functionality to make this easy:","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157981_677141355","id":"20190716-093311_1567955121","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5658"},{"text":"%python\nfrom sklearn.preprocessing import OneHotEncoder\n\n# read files\ncols = ['delay', 'month', 'day', 'dow', 'hour', 'distance', 'carrier', 'dest', 'days_from_holiday']\ncol_types = {'delay': int, 'month': int, 'day': int, 'dow': int, 'hour': int, 'distance': int, \n             'carrier': str, 'dest': str, 'days_from_holiday': int}\ndata_2007 = read_csv_from_hdfs('airline/fm/ord_2007_1', cols, col_types)\ndata_2008 = read_csv_from_hdfs('airline/fm/ord_2008_1', cols, col_types)\n\n# Create training set and test set\ntrain_y = data_2007['delay'] >= 15\ncateg = [cols.index(x) for x in ('hour', 'month', 'day', 'dow', 'carrier', 'dest')]\nenc = OneHotEncoder(categorical_features = categ)\ndf = data_2007.drop('delay', axis=1)\ndf['carrier'] = pd.factorize(df['carrier'])[0]\ndf['dest'] = pd.factorize(df['dest'])[0]\ntrain_x = enc.fit_transform(df)\n\ntest_y = data_2008['delay'] >= 15\ndf = data_2008.drop('delay', axis=1)\ndf['carrier'] = pd.factorize(df['carrier'])[0]\ndf['dest'] = pd.factorize(df['dest'])[0]\ntest_x = enc.transform(df)\n\nprint(train_x.shape)","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157981_677141355","id":"20190715-173833_689674818","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:51:53-0400","dateFinished":"2020-07-09T21:51:54-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5659"},{"text":"%md\nNow we have ~359K rows and 409 (!) features in our model. Let's re-run the Random Forest model and see if this improved our model:\n","dateUpdated":"2020-07-09T23:03:02-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157981_677141355","id":"20190716-093442_181156229","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5660","user":"deadline","dateFinished":"2020-07-09T23:03:02-0400","dateStarted":"2020-07-09T23:03:02-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now we have ~359K rows and 409 (!) features in our model. Let&rsquo;s re-run the Random Forest model and see if this improved our model:</p>\n</div>"}]}},{"text":"%python\n# Create Random Forest classifier with 50 trees\nclf_rf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\nclf_rf.fit(train_x.toarray(), train_y)\n\n# Evaluate on test set\npr = clf_rf.predict(test_x.toarray())\n\n# print results\ncm = confusion_matrix(test_y, pr)\nprint(\"Confusion matrix\")\nprint(pd.DataFrame(cm))\nreport_svm = precision_recall_fscore_support(list(test_y), list(pr), average='binary')\nprint(\"\\nprecision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n        (report_svm[0], report_svm[1], report_svm[2], accuracy_score(list(test_y), list(pr))))","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157982_678295602","id":"20190715-174225_1884403148","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T21:52:08-0400","dateFinished":"2020-07-09T21:52:58-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5661"},{"text":"%md\nThis clearly helped -- accuracy is higher at ~71%, and true positive are also better at 216K (vs 197K previously). (notice that the run took longer due to more features)","user":"deadline","dateUpdated":"2020-07-09T23:03:14-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157982_678295602","id":"20190716-093626_1846011644","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:03:14-0400","dateFinished":"2020-07-09T23:03:14-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5662","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This clearly helped &ndash; accuracy is higher at ~71%, and true positive are also better at 216K (vs 197K previously). (notice that the run took longer due to more features)</p>\n</div>"}]}},{"text":"%md\n### Logic Regression with PySpark ###\nStart with the cleaned prepocessed data (airline/fm/ord_2007_1 and airline/fm/ord_2008_1) and develop a logical regression model using PySpark\nThere are comments in the code that begin with `# VIEW`. The lines after this comment can be \"uncommented\" and used to view the data as the analysis progresses.\n\n**Note:** Using Spark2 and Python 3 (The PySpark V1 Logical Regression model does not seem to work correctly)\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157983_677910853","id":"20190816-133602_771523531","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5663"},{"text":"%spark.pyspark\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import IntegerType\nfrom itertools import islice\n\n# A user defined function (UDF) that adjusts dealys (greater then 15 minutes=1.0, less=0.0)\ndef delay(d):\n        if (d >= 15): return 1.0\n        else: return 0.0\n\n#report Python version, Spark2 uses Python 3\nimport sys\nprint(\"Python Version:\")\nprint(sys.version)\n\n# read preprocessed data into RDD, split on commas\nrdd_2007 = sc.textFile('airline/fmatrix/ord_2007_1').map(lambda p: p.split(\",\"))\nrdd_2008 = sc.textFile('airline/fmatrix/ord_2008_1').map(lambda p: p.split(\",\"))\n\n# VIEW check first ten listings\n# for y in islice(rdd_2007.collect(), 10): print y\n\n# the columns that are used include delay, month, day, dow, hour, distance, carrier, dest, days_from_holiday\n# only read numberic values and import to a PySpark DataFrame\ndf_2007_raw = rdd_2007.map(lambda p: Row(delay = int(p[0]), month = int(p[1]), day=int(p[2]), dow=int(p[3]), hour=int(p[4]), distance=int(p[5]),dfh=int(p[8])  )).toDF()\ndf_2008_raw = rdd_2008.map(lambda p: Row(delay = int(p[0]), month = int(p[1]), day=int(p[2]), dow=int(p[3]), hour=int(p[4]), distance=int(p[5]),dfh=int(p[8])  )).toDF()\n\n# VIEW print dataframe and schema\n# df_2008_raw.printSchema()\ndf_2007_raw.show(5)\n\nadjust_delay=UserDefinedFunction(delay,DoubleType())\n\ndf_2007 = df_2007_raw.withColumn('delay', adjust_delay(df_2007_raw.delay))\ndf_2008 = df_2008_raw.withColumn('delay', adjust_delay(df_2008_raw.delay))\n\n# VIEW inspect the changes to delay column\n# df_2007.show(5)\n#df_2008.show(5)","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":334,"optionOpen":false}}},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"tableHide":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157983_677910853","id":"20190816-133616_193341461","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T22:08:40-0400","dateFinished":"2020-07-09T22:08:44-0400","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5664"},{"text":"%md\n\n### Prepare Training Data ###\nThere are three steps:\n\n1. The VectorAssember - creates a single feature column with all features that will be used by the model\n2. The StandardScaler - standardizes a set of features to have zero mean and a standard deviation of 1. Scaled data can improve the convergence rate during the optimization process, and also prevents against features with very large variances exerting an overly large influence during model training.\n3. The StringIndexer - adds a new column \"called label\" to the DataFrame. The data in the label column comes from the delay data.\n\nUncomment the statements under the `# VIEW` comments to see progression\n","dateUpdated":"2020-07-09T23:03:25-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157983_677910853","id":"20190819-081854_32864940","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5665","user":"deadline","dateFinished":"2020-07-09T23:03:25-0400","dateStarted":"2020-07-09T23:03:25-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Prepare Training Data</h3>\n<p>There are three steps:</p>\n<ol>\n  <li>The VectorAssember - creates a single feature column with all features that will be used by the model</li>\n  <li>The StandardScaler - standardizes a set of features to have zero mean and a standard deviation of 1. Scaled data can improve the convergence rate during the optimization process, and also prevents against features with very large variances exerting an overly large influence during model training.</li>\n  <li>The StringIndexer - adds a new column &ldquo;called label&rdquo; to the DataFrame. The data in the label column comes from the delay data.</li>\n</ol>\n<p>Uncomment the statements under the <code># VIEW</code> comments to see progression</p>\n</div>"}]}},{"text":"%spark2.pyspark\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import StringIndexer\n\n# These are the \"feature\" columns\ncols = ['month', 'day', 'dow', 'hour', 'distance', 'dfh']\n\n# set the assembert to use \"cols\" and output to new \"features\" column\n# Also note, converts everthing to floats\nassembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\ndf_2007_va = assembler.transform(df_2007)\ndf_2008_va = assembler.transform(df_2008)\n\n# VIEW first 5 lines of the new DataFrame,\"False\" indicates no column truncation.\ndf_2007_va.show(5,False)\n#df_2008_va.show(5, False)\n\n# Normalize each feature to have unit standard deviation.\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",withStd=True, withMean=True)\ndf_2007_scaled_train = scaler.fit(df_2007_va).transform(df_2007_va)\ndf_2008_scaled_test = scaler.fit(df_2008_va).transform(df_2008_va)\n\n# VIEW first 5 lines of the new scaled DataFrame\ndf_2007_scaled_train.show(5,False)\n#df_2008_scaled_test.show(5,False)\n\n# Use StringIndexer to add \"label\" column using the \"delay\" data\nlabel_Indexer = StringIndexer(inputCol = 'delay', outputCol = 'label')\ndf_2007_scaled_train_label = label_Indexer.fit(df_2007_scaled_train).transform(df_2007_scaled_train)\ndf_2008_scaled_test_label = label_Indexer.fit(df_2008_scaled_test).transform(df_2008_scaled_test)\n\n#VIEW The final DataFRAMES ready for the model\ndf_2007_scaled_train_label.show(5)\n#df_2008_scaled_test_label.show(5)\n                       ","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python"},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157984_663675144","id":"20190816-151706_34127126","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5666"},{"text":"%md\n### Run and Test the Model\n\nWe use the LogisticRegression model from mlib. This model works with DataFrames. \n","dateUpdated":"2020-07-09T23:03:30-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157984_663675144","id":"20190819-085228_756900292","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5667","user":"deadline","dateFinished":"2020-07-09T23:03:30-0400","dateStarted":"2020-07-09T23:03:30-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Run and Test the Model</h3>\n<p>We use the LogisticRegression model from mlib. This model works with DataFrames.</p>\n</div>"}]}},{"text":"%spark2.pyspark\n\n# utility to print confusion matrix and metrics\ndef print_cm(tp,tn,fn,fp):\n    print(\"\\nConfusion Matrix\")\n    print(\"\\n           Prediction\")\n    print(\"\\n             0        1\")\n    print(\"\\n Actual 0  %d | %d\" % (tp,fn))\n    print(\"\\n          -----------------\")\n    print(\"\\n        1  %d | %d\" % (fp,tn))\n    a=(tp+tn)/(tp+tn+fp+fn)\n    p=tp/(tp+fp)\n    r=tp/(tp+fn)\n    f1=2*(p*r/(p+r))  \n    print(\"\\n   \")\n    print(\"\\n\\nprecision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % (p,r,f1,a))\n    print(\"\\nCount total = %d, Correct = %d, Incorrect = %d\" % (tp+tn+fp+fn, tp+tn,fp+fn))\n    print(\"\\nTrue Positives = %d, True Negatives = %d, False Negatives = %d, False Positives = %d\" % (tp,tn,fn,fp))\n    return\n","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157985_663290395","id":"20190820-135048_150430299","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-08T15:02:58-0400","dateFinished":"2020-07-08T15:02:59-0400","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5668"},{"text":"%spark2.pyspark\nfrom pyspark.ml.classification import LogisticRegression\n\n# run the model using scaledFeatures and label column\nlr = LogisticRegression(featuresCol = 'scaledFeatures', labelCol = 'label', maxIter=25)\nlrModel = lr.fit(df_2007_scaled_train_label)\n\n# test the result\npredict_test=lrModel.transform(df_2008_scaled_test_label)\n\n# VIEW inspect resulting prediction\n#predict_test.show(5)\n#predict_test.select(\"label\",\"prediction\").show(10,False)\n\n# Calculate metrics from results\n# times delay occured and was predicted correctly\ntrueN = predict_test.filter(predict_test.prediction == 1.0).filter( predict_test.label ==  predict_test.prediction).count()\n# times no-delay occured and was predicted correctly\ntrueP = predict_test.filter(predict_test.prediction == 0.0).filter( predict_test.label ==  predict_test.prediction).count()\n# times delay occured and was not predicted\nfalseN =  predict_test.filter(predict_test.prediction == 1.0).filter(predict_test.label != predict_test.prediction).count()\n# times there was no-delay and delay was predicted\nfalseP =  predict_test.filter(predict_test.prediction == 0.0).filter(predict_test.label != predict_test.prediction).count()\n\n# print confustion matrix\nprint_cm(trueP,trueN,falseN,falseP)\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157985_663290395","id":"20190818-124500_1972656133","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5669"},{"text":"%md\n### Save and Restore the Model\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157985_663290395","id":"20190822-165040_1463974121","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5670"},{"text":"%spark2.pyspark\n# Save the model to HDFS\nlrModel.save(\"lr-pyspark-model\")\n\nfrom pyspark.ml.classification import LogisticRegressionModel\n# load it back in from HDFS\nlrModel_copy = LogisticRegressionModel.load(\"lr-pyspark-model\")\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python"},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157986_664444642","id":"20190822-162933_1130777320","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5671"},{"text":"%md\nThe PySpark model provides good results, however there are a lot of false positives. This result means we predict no-delay when there were actualy delay. Some more work on the model is required. \n","dateUpdated":"2020-07-09T23:03:40-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157986_664444642","id":"20190819-090544_1837670598","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5672","user":"deadline","dateFinished":"2020-07-09T23:03:40-0400","dateStarted":"2020-07-09T23:03:40-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The PySpark model provides good results, however there are a lot of false positives. This result means we predict no-delay when there were actualy delay. Some more work on the model is required.</p>\n</div>"}]}},{"text":"%md\n### Adding Weather Data (Iteration 3) ###\n\n(Back to SKLearn approach)\n\nAnother common path to improve accuracy is by bringing in new types of data - enriching our dataset - and generating more features. Our idea is to layer-in weather data. We can get this data from a publicly available dataset [here] (http://www.ncdc.noaa.gov/cdo-web/datasets/). See above paragraph for downloading data.\n\nWe will look at daily temperatures (min/max), wind speed, snow conditions and precipitation in the flight origin airport (ORD). Clearly, weather conditions in the destination airport also affect delays, but for simplicity of this demo we just include weather at the origin (ORD).\n\nFirst, let's re-write our Pig original script to add these new features to our feature matrix:","dateUpdated":"2020-07-09T23:03:43-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157987_664059893","id":"20190716-093812_1095272170","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5673","user":"deadline","dateFinished":"2020-07-09T23:03:43-0400","dateStarted":"2020-07-09T23:03:43-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Adding Weather Data (Iteration 3)</h3>\n<p>(Back to SKLearn approach)</p>\n<p>Another common path to improve accuracy is by bringing in new types of data - enriching our dataset - and generating more features. Our idea is to layer-in weather data. We can get this data from a publicly available dataset <a href=\"http://www.ncdc.noaa.gov/cdo-web/datasets/\">here</a>. See above paragraph for downloading data.</p>\n<p>We will look at daily temperatures (min/max), wind speed, snow conditions and precipitation in the flight origin airport (ORD). Clearly, weather conditions in the destination airport also affect delays, but for simplicity of this demo we just include weather at the origin (ORD).</p>\n<p>First, let&rsquo;s re-write our Pig original script to add these new features to our feature matrix:</p>\n</div>"}]}},{"text":"%sh\n# user heredoc to local directory. Run as \"hive -f hive-script-3.hql\"\n cat << EOF >hive-script-3.hql\n -- SCRIPT 3 CREATE FEATURE MATRIX AND INCLUDE WEATHER DATA\n-- SAVE AS CSV FILE FOR BOTH 2007 and 2008\n\n-- creat external tables for weather data\n\nCREATE EXTERNAL TABLE weather_2007 \n(station string, wdate string, metric string, value int, t1 int, t2 int, t3 int, time string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE\nLOCATION '/data/weather/2007'\nTBLPROPERTIES ('external.table.purge'='false');\n\nCREATE EXTERNAL TABLE weather_2008 \n(station string, wdate string, metric string, value int, t1 int, t2 int, t3 int, time string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE\nLOCATION '/data/weather/2008'\nTBLPROPERTIES ('external.table.purge'='false');\n\n-- Create raw table for data we want\nCREATE TABLE weather_2007_raw (wdate string, metric string, value int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE weather_2008_raw (wdate string, metric string, value int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- Insert the data, select the weatehr station closest to ORD\nINSERT INTO TABLE weather_2007_raw(wdate, metric, value)\nSELECT wdate, metric, value FROM weather_2007\nWHERE station='USW00094846';\n\nINSERT INTO TABLE weather_2008_raw(wdate, metric, value)\nSELECT wdate, metric, value FROM weather_2008\nWHERE station='USW00094846';\n\n-- Check data\nSELECT COUNT(*) FROM weather_2007_raw;\nSELECT * FROM weather_2007_raw LIMIT 20;\nSELECT COUNT(*) FROM weather_2008_raw;\nSELECT * FROM weather_2008_raw LIMIT 20;\n\n-- We need to pivot the data into a new table that keep all\n-- data by date\nCREATE TABLE weather_2007_pivot (wdate string, tmax int, tmin int, prcp int, snow int, awnd int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE weather_2008_pivot (wdate string, tmax int, tmin int, prcp int, snow int, awnd int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- Insert the data\nINSERT INTO TABLE weather_2007_pivot(wdate, tmax, tmin, prcp, snow, awnd)\nSELECT wdate, \nsum(case when metric='TMAX' then value end) as tmax, \nsum(case when metric='TMIN' then value end) as tmin, \nsum(case when metric='PRCP' then value end) as prcp, \nsum(case when metric='SNOW' then value end) as snow, \nsum(case when metric='AWND' then value end) as awnd \nFROM weather_2007_raw\nGROUP BY wdate;\n\nINSERT INTO TABLE weather_2008_pivot(wdate, tmax, tmin, prcp, snow, awnd)\nSELECT wdate, \nsum(case when metric='TMAX' then value end) as tmax, \nsum(case when metric='TMIN' then value end) as tmin, \nsum(case when metric='PRCP' then value end) as prcp, \nsum(case when metric='SNOW' then value end) as snow, \nsum(case when metric='AWND' then value end) as awnd \nFROM weather_2008_raw\nGROUP BY wdate;\n\n-- Next join flight data and wather data into new feature matrix\n-- First crate new fmatrix_raw data with a date field so it\n-- can be related to the weather data, we include this in\n-- the Hive paython mapper\n-- Create the table\n\nCREATE TABLE fmatrix_2007_raw_2 (delay int, year int, month int, day int, dayofweek int, hour_mins string, distance int, carrier string, dest string, days_from_holiday int, fdate string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE fmatrix_2008_raw_2 (delay int, year int, month int, day int, dayofweek int, hour_mins string, distance int, carrier string, dest string, days_from_holiday int, fdate string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- Pull raw data from the external Hive table\nINSERT INTO TABLE fmatrix_2007_raw_2(delay, year, month, day, dayofweek, hour_mins, distance, carrier,dest)\nSELECT depdelay, year, month, dayofmonth, dayofWeek, crsdeptime,distance, uniquecarrier, dest FROM flights_2007\nWHERE origin='ORD' and cancelled=0 and depdelay is not NULL and depdelay>0;\n\nINSERT INTO TABLE fmatrix_2008_raw_2(delay, year, month, day, dayofweek, hour_mins, distance, carrier,dest)\nSELECT depdelay, year, month, dayofmonth, dayofWeek, crsdeptime,distance, uniquecarrier, dest FROM flights_2008\nWHERE origin='ORD' and cancelled=0 and depdelay is not NULL and depdelay>0;\n\n-- Like before we need to convert, the time of day (from hour_mins, to hour), \n-- calculate the days_from_holiday, and create a full date feild to match the \n-- the weather data (YYYYMMDD). Create new table for transformed results\nCREATE TABLE fmatrix_2007_2 (delay int, month int, day int, dayofweek int, hour string, distance int, carrier string, dest string, days_from_holiday int, fdate string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE fmatrix_2008_2 (delay int, month int, day int, dayofweek int, hour string, distance int, carrier string, dest string, days_from_holiday int, fdate string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- Add the python file, note if you need to change this file\n-- It is different file from the other script\n-- delete and then reload it \n-- delete FILE feature-gen2.py\nadd FILE feature-gen2.py;\n\n-- convert table \nINSERT OVERWRITE TABLE fmatrix_2007_2\nSELECT\n  TRANSFORM (delay, year, month, day, dayofweek, hour_mins, distance, carrier, dest, days_from_holiday,fdate)\n  USING 'python feature-gen2.py'\n  AS (delay, month, day, dayofweek, hour, distance, carrier, dest, days_from_holiday, fdate)\nFROM fmatrix_2007_raw_2;\n\nINSERT OVERWRITE TABLE fmatrix_2008_2\nSELECT\n  TRANSFORM (delay, year, month, day, dayofweek, hour_mins, distance, carrier, dest, days_from_holiday,fdate)\n  USING 'python feature-gen2.py'\n  AS (delay, month, day, dayofweek, hour, distance, carrier, dest, days_from_holiday, fdate)\nFROM fmatrix_2008_raw_2;\n\n-- Check data\nSELECT COUNT(*) FROM fmatrix_2007_2;\nSELECT * FROM fmatrix_2007_2 LIMIT 5;\nSELECT COUNT(*) FROM fmatrix_2008_2;\nSELECT * FROM fmatrix_2008_2 LIMIT 5;\n\n-- Merge both tables into final feature matrix\n-- Create the new table\nCREATE TABLE fmatrix_2007_Weather (delay int, month int, day int, dayofweek int, hour string, distance int, carrier string, dest string, days_from_holiday int, tmin int, tmax int, prcp int, snow int, awnd int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\nCREATE TABLE fmatrix_2008_Weather (delay int, month int, day int, dayofweek int, hour string, distance int, carrier string, dest string, days_from_holiday int, tmin int, tmax int, prcp int, snow int, awnd int)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n\n-- Join the data (adds weather data for each flight)\nINSERT OVERWRITE TABLE fmatrix_2007_Weather\nSELECT o.delay, o.month, o.day, o.dayofweek, o.hour, o.distance, o.carrier, o.dest, o.days_from_holiday, c.tmax, c.tmin, c.prcp, c.snow, c.awnd \nFROM weather_2007_pivot c JOIN fmatrix_2007_2 o \nON (c.wdate = o.fdate);\n\nINSERT OVERWRITE TABLE fmatrix_2008_Weather\nSELECT o.delay, o.month, o.day, o.dayofweek, o.hour, o.distance, o.carrier, o.dest, o.days_from_holiday, c.tmax, c.tmin, c.prcp, c.snow, c.awnd \nFROM weather_2008_pivot c JOIN fmatrix_2008_2 o \nON (c.wdate = o.fdate);\n\n-- Check data\nSELECT COUNT(*) FROM fmatrix_2007_Weather;\nSELECT * FROM fmatrix_2007_Weather LIMIT 5;\nSELECT COUNT(*) FROM fmatrix_2008_Weather;\nSELECT * FROM fmatrix_2008_Weather LIMIT 5;\n\n-- save hive table as csv file\n\nINSERT OVERWRITE DIRECTORY 'airline/fmatrix/ord_2007_2' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM fmatrix_2007_Weather ;\nINSERT OVERWRITE DIRECTORY 'airline/fmatrix/ord_2008_2' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM fmatrix_2008_Weather ;\nEOF\n\n","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/sh","results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157987_664059893","id":"20190715-190025_1460200128","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5674"},{"text":"%md\nWe now read this data in, convert temperatures to Fahrenheit (note original temp is in Celcius*10), and prepare the training and testing datasets for OHE  modeling.\n","dateUpdated":"2020-07-09T23:03:51-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157987_664059893","id":"20190716-094342_759493363","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5675","user":"deadline","dateFinished":"2020-07-09T23:03:51-0400","dateStarted":"2020-07-09T23:03:51-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We now read this data in, convert temperatures to Fahrenheit (note original temp is in Celcius*10), and prepare the training and testing datasets for OHE modeling.</p>\n</div>"}]}},{"text":"%python\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Convert Celsius to Fahrenheit\ndef fahrenheit(x): return(x*1.8 + 32.0)\n\n# read files\ncols = ['delay', 'month', 'day', 'dow', 'hour', 'distance', 'carrier', 'dest', 'days_from_holiday',\n        'origin_tmin', 'origin_tmax', 'origin_prcp', 'origin_snow', 'origin_wind']\ncol_types = {'delay': int, 'month': int, 'day': int, 'dow': int, 'hour': int, 'distance': int, \n             'carrier': str, 'dest': str, 'days_from_holiday': int,\n             'origin_tmin': float, 'origin_tmax': float, 'origin_prcp': float, 'origin_snow': float, 'origin_wind': float}\n\ndata_2007 = read_csv_from_hdfs('airline/fmatrix/ord_2007_2', cols, col_types)\ndata_2008 = read_csv_from_hdfs('airline/fmatrix/ord_2008_2', cols, col_types)\n\ndata_2007['origin_tmin'] = data_2007['origin_tmin'].apply(lambda x: fahrenheit(x/10.0))\ndata_2007['origin_tmax'] = data_2007['origin_tmax'].apply(lambda x: fahrenheit(x/10.0))\ndata_2008['origin_tmin'] = data_2008['origin_tmin'].apply(lambda x: fahrenheit(x/10.0))\ndata_2008['origin_tmax'] = data_2008['origin_tmax'].apply(lambda x: fahrenheit(x/10.0))\n\n# Create training set and test set\ntrain_y = data_2007['delay'] >= 15\ncateg = [cols.index(x) for x in ('hour', 'month', 'day', 'dow', 'carrier', 'dest')]\nenc = OneHotEncoder(categorical_features = categ)\ndf = data_2007.drop('delay', axis=1)\ndf['carrier'] = pd.factorize(df['carrier'])[0]\ndf['dest'] = pd.factorize(df['dest'])[0]\ntrain_x = enc.fit_transform(df)\n\ntest_y = data_2008['delay'] >= 15\ndf = data_2008.drop('delay', axis=1)\ndf['carrier'] = pd.factorize(df['carrier'])[0]\ndf['dest'] = pd.factorize(df['dest'])[0]\ntest_x = enc.transform(df)\n\nprint(train_x.shape)","user":"deadline","dateUpdated":"2020-07-09T23:01:18-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157988_662136148","id":"20190715-192520_1378207123","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T22:03:21-0400","dateFinished":"2020-07-09T22:03:22-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5676"},{"text":"%md\nNext, rerun the training set with new features. Update the number of trees to 100.\n","user":"deadline","dateUpdated":"2020-07-09T23:03:57-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157988_662136148","id":"20190716-094427_1775537670","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:03:57-0400","dateFinished":"2020-07-09T23:03:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5677","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Next, rerun the training set with new features. Update the number of trees to 100.</p>\n</div>"}]}},{"text":"%python\n# Create Random Forest classifier with 100 trees\nclf_rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nclf_rf.fit(train_x.toarray(), train_y)\n\n# Evaluate on test set\npr = clf_rf.predict(test_x.toarray())\n\n# print results\ncm = confusion_matrix(test_y, pr)\nprint(\"Confusion matrix\")\nprint(pd.DataFrame(cm))\nreport_rf = precision_recall_fscore_support(list(test_y), list(pr), average='binary')\nprint(\"precision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n        (report_rf[0], report_rf[1], report_rf[2], accuracy_score(list(test_y), list(pr))))","user":"deadline","dateUpdated":"2020-07-09T23:01:19-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157989_661751399","id":"20190715-211557_1640139280","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T22:03:54-0400","dateFinished":"2020-07-09T22:04:31-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5678"},{"text":"%md\nThe results are not as good with the weather data, more experimentation is needed! Our best was 72% \n\nLet's save the model for now\n","user":"deadline","dateUpdated":"2020-07-09T23:04:03-0400","config":{"enabled":true,"tableHide":false,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157989_661751399","id":"20190715-212743_1416883405","dateCreated":"2020-07-08T14:15:57-0400","dateStarted":"2020-07-09T23:04:03-0400","dateFinished":"2020-07-09T23:04:03-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5679","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The results are not as good with the weather data, more experimentation is needed! Our best was 72% </p>\n<p>Let&rsquo;s save the model for now</p>\n</div>"}]}},{"text":"%python\n# use pickle serializer/deserializer\nimport pickle\n\n# save the model to local file system\nsave_filename = 'clf_rf_model_v1.sav'\npickle.dump(clf_rf, open(save_filename, 'wb'))\n\n# load the model from local file system\nload_filename = 'clf_rf_model_v1.sav'\nclf_rf_copy = pickle.load(open(load_filename, 'rb'))\n","dateUpdated":"2020-07-09T23:01:19-0400","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157989_661751399","id":"20190823-095403_2139562472","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5680"},{"text":"%md\n### Conclusion ###\nThis concludes the Data Science Notebook. Updates will include fixing the issues found above and additional tools (including pyspark)\n\n","dateUpdated":"2020-07-09T23:04:06-0400","config":{"enabled":true,"editorMode":"ace/mode/markdown","results":{},"editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157990_662905646","id":"20190716-094819_13555703","dateCreated":"2020-07-08T14:15:57-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5681","user":"deadline","dateFinished":"2020-07-09T23:04:06-0400","dateStarted":"2020-07-09T23:04:06-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Conclusion</h3>\n<p>This concludes the Data Science Notebook. Updates will include fixing the issues found above and additional tools (including pyspark)</p>\n</div>"}]}},{"text":"%md\n","dateUpdated":"2020-07-09T23:01:19-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1594232157990_662905646","id":"20190716-094953_1222092705","dateCreated":"2020-07-08T14:15:57-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5682"}],"name":"Scalable Analytics Airline Delays V2","id":"2FFD4SP4A","angularObjects":{"2EFNH2N1T:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2EF3MC3PD:shared_process":[],"2CUSV2RC8:shared_process":[],"2CXH6CR6P:shared_process":[],"2CW1CJHSN:shared_process":[],"2CXFTT4PT:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CXAF2UNW:shared_process":[],"2CX3VRR2H:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}